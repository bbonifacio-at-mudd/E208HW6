{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35698731",
   "metadata": {},
   "source": [
    "# Team Viviane Solomon and Brandon Bonifacio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930f78e",
   "metadata": {},
   "source": [
    "# HW6: Life Cycle of ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfb03a",
   "metadata": {},
   "source": [
    "Your goal is to develop a CNN model that, given a cell phone image\n",
    "taken somewhere inside a building on HMC campus, can identify which building is being\n",
    "photographed. You may use any online resources that you find helpful, but you must cite your\n",
    "sources and indicate clearly what portions of your code have been copied and modified from\n",
    "elsewhere. You may work individually or with a partner on this assignment. Please submit your\n",
    "assignment as a single jupyter notebook on Sakai. If you work with a partner, make sure to\n",
    "indicate both partners’ names clearly at the very top of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d51d",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection/Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2489d",
   "metadata": {},
   "source": [
    "In the first part of the assignment, you will do the following:\n",
    "\n",
    "\n",
    "• Data Collection (15 points). Each student/team will collect 50 cell phone pictures taken\n",
    "of random locations inside a single building on campus. Please sign up for a building to\n",
    "photograph in this spreadsheet, and upload your pictures to this shared google drive.\n",
    "Since the data will be used by the entire class, please complete this portion of the\n",
    "assignment by Saturday 1pm (-5 points if not done by then). If there are more than 5\n",
    "buildings represented in the class data, you may simply select 5 buildings to use for this\n",
    "assignment.\n",
    "\n",
    "\n",
    "• Data Preparation (15 points). Download the class data onto your laptop. Prepare the\n",
    "data for use in PyTorch by ensuring image format compatibility, putting the images in a\n",
    "suitable directory structure, and creating train & validation partitions. Describe your\n",
    "data preparation process in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5eef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef55a4",
   "metadata": {},
   "source": [
    "### In the cells below, we prepare the data for use in PyTorch by ensuring image format compatiblity, putting the images in a suitable directory structure, and creating train & validation partitions. We describe our data preparation process in the markdown cell below, and then encode it after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88548f",
   "metadata": {},
   "source": [
    "***Input description here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_data():\n",
    "    \"\"\"\n",
    "    This function loads and partitions our image data.\n",
    "    \"\"\"\n",
    "    #EXAMPLE MNIST CODE:\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    X_train = np.array(mnist_trainset.data.reshape(len(mnist_trainset), -1), dtype = np.float64)/255\n",
    "    Y_train = pd.get_dummies(np.array(mnist_trainset.targets)).to_numpy(dtype = np.float64) # one-hot encoding\n",
    "\n",
    "    mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "    X_test = np.array(mnist_testset.data.reshape(len(mnist_testset), -1), dtype = np.float64)/255\n",
    "    Y_test = pd.get_dummies(np.array(mnist_testset.targets)).to_numpy(dtype = np.float64)\n",
    "    \n",
    "    return return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5690d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b30cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = X_train[0,:].reshape((28,28))\n",
    "plt.imshow(sample_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdc98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f69efc",
   "metadata": {},
   "source": [
    "## Part 2: Nearest Neighbors Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f1cee",
   "metadata": {},
   "source": [
    "In the second part of the assignment, you will do the following:\n",
    "\n",
    "• Feature extraction (15 points). Find a pretrained CNN model (e.g. ResNet) and use the\n",
    "penultimate layer activations as a feature representation. Your jupyter notebook should\n",
    "include code that demonstrates how to use the pretrained model to extract features\n",
    "from an image in the dataset.\n",
    "\n",
    "\n",
    "• Nearest Neighbor Method (15 points). Extract features from all the images in the\n",
    "training set and store them in a single file along with the building labels. For each image\n",
    "in the validation set, use the pretrained CNN model to extract the feature\n",
    "representation, calculate which training image is closest in Euclidean distance, and use\n",
    "its label as the prediction. Report your classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761da41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54546893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet():\n",
    "    \"\"\"\n",
    "    \n",
    "    Loads a pretrained ResNet model. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = ##\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73960b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (62451961.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_46904\\62451961.py\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    features = ##\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def feature_extraction(model, data):\n",
    "    \"\"\"\n",
    "    \n",
    "    Uses a pretrained model and returns the penultimate layer activations \n",
    "    as a feature representation. \n",
    "    \n",
    "    This code demonstrates how to use a pretrained model to extract features from an image in the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #We have 2 cases. The first case is if we have an array of images\n",
    "    #The second case is if we only have one image\n",
    "    \n",
    "    try: #Case 1: Array of images\n",
    "        m = len(data) #This will fail if it is case 2\n",
    "        \n",
    "        features = ##\n",
    "    except: #Case 2: Single image\n",
    "        \n",
    "        \n",
    "        features = ##\n",
    "        \n",
    "    return features\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = feature_extraction(model, X_train)\n",
    "X_val_features = feature_extraction(model, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406ef2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(X_train_features, Y_train, data_features):\n",
    "    \"\"\"\n",
    "    \n",
    "    Here, we calculate which training image is closest in Euclidean distance to each given validation image,\n",
    "    and we use that to label the training image. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try: #Case 1: Array of images\n",
    "        m = len(data_features) #This will fail if it is case 2\n",
    "        \n",
    "        labels = ##\n",
    "    except: #Case 2: Single image\n",
    "        \n",
    "        \n",
    "        labels = ##\n",
    "    \n",
    "    \n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = nearest_neighbors(X_train_features, Y_train, X_val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fd964d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46904\\2188396818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Our classification accuracy using a nearest-single-neighbor approach is {accuracy:.2f}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_labels' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = 100*np.mean(val_labels==np.argmax(Y_val, axis=1))\n",
    "print(f\"Our classification accuracy using a nearest-single-neighbor approach is {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b32b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f43108",
   "metadata": {},
   "source": [
    "## Part 3: Fine-tuning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c255a1",
   "metadata": {},
   "source": [
    "In the third part of the assignment, you will do the following:\n",
    "\n",
    "\n",
    "• Finetuning (25 points). Remove the output layer of the pretrained CNN model and\n",
    "replace it with a randomly initialized output layer that classifies among the 5 buildings of\n",
    "interest. Finetune the modified model on the training samples. Include your\n",
    "training/validation loss curves in your notebook, along with the final validation\n",
    "classification accuracy.\n",
    "\n",
    "\n",
    "• Improvements (10 points). Experiment with different ways to improve the validation\n",
    "accuracy. Include any results or figures to document your progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0740213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_resnet():\n",
    "    \"\"\"\n",
    "    \n",
    "    Loads a pretrained ResNet model, but with the final output layer removed and replaced with a randomly \n",
    "    initialized output layer that will classify among the 5 buildings. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = ##\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0025018",
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_model = load_random_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dcd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, X_train, Y_train, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    \n",
    "    Trains the given model on the X_train and Y_train data, and returns the trained model as well as the losses\n",
    "    and final validation accuracy.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    hist = [[], []] #Histogram of training and validation losses\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Train the model\n",
    "    \n",
    "    \n",
    "    return model, hist, final_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, hist, final_val_accuracy = finetune(model, X_train, Y_train, X_val, Y_val)\n",
    "print(f\"Our final validation accuracy accuracy using the full CNN is {final_val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45de4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4583c481",
   "metadata": {},
   "source": [
    "## Below are some of the different ways we experimented with to improve accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad77f5",
   "metadata": {},
   "source": [
    "### Below I show the results of the augmentation approach. Using this method, we were able to improve the accuracy by _%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_data_augmentation():\n",
    "    \"\"\"\n",
    "    This function loads and partitions our image data, and it also performs data augmentation to try to improve results. \n",
    "    \"\"\"\n",
    "    #EXAMPLE MNIST CODE:\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    X_train = np.array(mnist_trainset.data.reshape(len(mnist_trainset), -1), dtype = np.float64)/255\n",
    "    Y_train = pd.get_dummies(np.array(mnist_trainset.targets)).to_numpy(dtype = np.float64) # one-hot encoding\n",
    "\n",
    "    mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "    X_test = np.array(mnist_testset.data.reshape(len(mnist_testset), -1), dtype = np.float64)/255\n",
    "    Y_test = pd.get_dummies(np.array(mnist_testset.targets)).to_numpy(dtype = np.float64)\n",
    "    \n",
    "    return return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug, Y_train_aug, X_val_aug, Y_val_aug = load_prepare_data_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14745d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = X_train_aug[0,:].reshape((28,28))\n",
    "plt.imshow(sample_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_model = load_random_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model, hist_aug, final_val_accuracy_aug = finetune(model_aug, X_train_aug, Y_train_aug, X_val_aug, Y_val_aug)\n",
    "print(f\"Our final validation accuracy accuracy using the full CNN with data augmentation is {final_val_accuracy_aug:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
