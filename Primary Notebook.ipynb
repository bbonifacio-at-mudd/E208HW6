{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbaa4bea",
   "metadata": {},
   "source": [
    "# Team Viviane Solomon and Brandon Bonifacio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2ad14",
   "metadata": {},
   "source": [
    "# HW6: Life Cycle of ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd03c23",
   "metadata": {},
   "source": [
    "Your goal is to develop a CNN model that, given a cell phone image\n",
    "taken somewhere inside a building on HMC campus, can identify which building is being\n",
    "photographed. You may use any online resources that you find helpful, but you must cite your\n",
    "sources and indicate clearly what portions of your code have been copied and modified from\n",
    "elsewhere. You may work individually or with a partner on this assignment. Please submit your\n",
    "assignment as a single jupyter notebook on Sakai. If you work with a partner, make sure to\n",
    "indicate both partners’ names clearly at the very top of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50d5ad",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection/Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bbc57",
   "metadata": {},
   "source": [
    "In the first part of the assignment, you will do the following:\n",
    "\n",
    "\n",
    "• Data Collection (15 points). Each student/team will collect 50 cell phone pictures taken\n",
    "of random locations inside a single building on campus. Please sign up for a building to\n",
    "photograph in this spreadsheet, and upload your pictures to this shared google drive.\n",
    "Since the data will be used by the entire class, please complete this portion of the\n",
    "assignment by Saturday 1pm (-5 points if not done by then). If there are more than 5\n",
    "buildings represented in the class data, you may simply select 5 buildings to use for this\n",
    "assignment.\n",
    "\n",
    "\n",
    "• Data Preparation (15 points). Download the class data onto your laptop. Prepare the\n",
    "data for use in PyTorch by ensuring image format compatibility, putting the images in a\n",
    "suitable directory structure, and creating train & validation partitions. Describe your\n",
    "data preparation process in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ede5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecc368",
   "metadata": {},
   "source": [
    "### In the cells below, we prepare the data for use in PyTorch by ensuring image format compatiblity, putting the images in a suitable directory structure, and creating train & validation partitions. We describe our data preparation process in the markdown cell below, and then encode it after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007821a",
   "metadata": {},
   "source": [
    "***Input description here***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_data():\n",
    "    \"\"\"\n",
    "    This function loads and partitions our image data.\n",
    "    \"\"\"\n",
    "    #EXAMPLE MNIST CODE:\n",
    "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    X_train = np.array(mnist_trainset.data.reshape(len(mnist_trainset), -1), dtype = np.float64)/255\n",
    "    Y_train = pd.get_dummies(np.array(mnist_trainset.targets)).to_numpy(dtype = np.float64) # one-hot encoding\n",
    "\n",
    "    mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "    X_test = np.array(mnist_testset.data.reshape(len(mnist_testset), -1), dtype = np.float64)/255\n",
    "    Y_test = pd.get_dummies(np.array(mnist_testset.targets)).to_numpy(dtype = np.float64)\n",
    "    \n",
    "    \n",
    "    #We should also try some data augmentation methods - simple stuff like adding random noise (there's packages for that)\n",
    "    \n",
    "    return return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e89b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaa15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = X_train[0,:].reshape((28,28))\n",
    "plt.imshow(sample_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b102b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bbabdaf",
   "metadata": {},
   "source": [
    "## Part 2: Nearest Neighbors Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6527c4",
   "metadata": {},
   "source": [
    "In the second part of the assignment, you will do the following:\n",
    "\n",
    "• Feature extraction (15 points). Find a pretrained CNN model (e.g. ResNet) and use the\n",
    "penultimate layer activations as a feature representation. Your jupyter notebook should\n",
    "include code that demonstrates how to use the pretrained model to extract features\n",
    "from an image in the dataset.\n",
    "\n",
    "\n",
    "• Nearest Neighbor Method (15 points). Extract features from all the images in the\n",
    "training set and store them in a single file along with the building labels. For each image\n",
    "in the validation set, use the pretrained CNN model to extract the feature\n",
    "representation, calculate which training image is closest in Euclidean distance, and use\n",
    "its label as the prediction. Report your classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d00a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a104425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet():\n",
    "    \"\"\"\n",
    "    \n",
    "    Loads a pretrained ResNet model. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model = ##\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d34f64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (62451961.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_46904\\62451961.py\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    features = ##\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def feature_extraction(model, data):\n",
    "    \"\"\"\n",
    "    \n",
    "    Uses a pretrained model and returns the penultimate layer activations \n",
    "    as a feature representation. \n",
    "    \n",
    "    This code demonstrates how to use a pretrained model to extract features from an image in the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #We have 2 cases. The first case is if we have an array of images\n",
    "    #The second case is if we only have one image\n",
    "    \n",
    "    try: #Case 1: Array of images\n",
    "        m = len(data) #This will fail if it is case 2\n",
    "        \n",
    "        features = ##\n",
    "    except: #Case 2: Single image\n",
    "        \n",
    "        \n",
    "        features = ##\n",
    "        \n",
    "    return features\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = feature_extraction(model, X_train)\n",
    "X_val_features = feature_extraction(model, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806bb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(X_train_features, Y_train, data_features):\n",
    "    \"\"\"\n",
    "    \n",
    "    Here, we calculate which training image is closest in Euclidean distance to each given validation image,\n",
    "    and we use that to label the training image. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try: #Case 1: Array of images\n",
    "        m = len(data_features) #This will fail if it is case 2\n",
    "        \n",
    "        labels = ##\n",
    "    except: #Case 2: Single image\n",
    "        \n",
    "        \n",
    "        labels = ##\n",
    "    \n",
    "    \n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = nearest_neighbors(X_train_features, Y_train, X_val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8cb091",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46904\\2188396818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Our classification accuracy using a nearest-single-neighbor approach is {accuracy:.2f}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_labels' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = 100*np.mean(val_labels==np.argmax(Y_val, axis=1))\n",
    "print(f\"Our classification accuracy using a nearest-single-neighbor approach is {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0992442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8352660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
